import matplotlib.pyplot as plt

# 可视化train loss

epochs = list(range(1, 101))
train_loss = [
    2.0928406704146907, 1.0797733231535498, 0.9489840527750412, 0.716003564449976, 0.6689919569301155,
    0.5699539557099342, 0.7553211080859292, 0.6655194295464821, 0.6525486934016336, 0.617345158397308,
    0.7101182231925568, 0.5356649222941894, 0.3845107358581615, 0.48536258545827193, 0.45981526764918046,
    0.5988623023665739, 0.4580654343414419, 0.5019797555188525, 0.5098033402958568, 0.6235024358655484,
    0.5540536194336865, 0.3921467769047562, 0.5160946938789116, 0.5268030458323236, 0.5581199432248777,
    0.41999077747734087, 0.56815584836844, 0.512985322905599, 0.451661159020831, 0.5217029116520623,
    0.49592003163020565, 0.5820833344785672, 0.46238860164610845, 0.5413202859180154, 0.4868155214232656,
    0.5672343133614873, 0.4823360633069895, 0.622104001003054, 0.5368572400074523, 0.5356311587581657,
    0.31010956002645335, 0.46105983238315806, 0.4572902892909522, 0.4747978065444051, 0.5180212337312833,
    0.4865493454777126, 0.3550312537172774, 0.403322018015216, 0.39831127864220794, 0.4782565802616893,
    0.3153028987031781, 0.3025385508454352, 0.5996956374355644, 0.3913214828045863, 0.4133179834955987,
    0.4141557219104384, 0.3877575792864246, 0.40304845961619096, 0.4728068131426314, 0.48011770006269217,
    0.4188325069844723, 0.428035764076378, 0.38904500380158424, 0.46398943654735975, 0.4041346245146585,
    0.4032711670382546, 0.5472601558134522, 0.3734461941211572, 0.3590657186852592, 0.5235271717615004,
    0.39040844388446716, 0.3189387171230507, 0.5108350914428538, 0.3870975386245914, 0.5007807264666794,
    0.346514125495165, 0.5342977030097313, 0.4509766804546399, 0.5430861350219205, 0.5591813149282111,
    0.507967870260747, 0.3748155037854921, 0.5060775375063971, 0.40235728686148264, 0.5780769937654149,
    0.40950466868168905, 0.4769488896577145, 0.36887223844609734, 0.44302808315897324, 0.420421204151902,
    0.435145579972568, 0.37293562602322056, 0.3887874599950353, 0.3420170069988466, 0.28807901510230777,
    0.5248925571488041, 0.4010317023175786, 0.45622954645879427, 0.37711114815945895, 0.4063037965833297
]
val_loss = [
    0.9854397488676984, 0.5049154654793118, 0.3826263773700465, 0.30485118666420813, 0.33760715478464315,
    0.27374178916215897, 0.218439187368621, 0.2133362285144951, 0.2560138767990081, 0.2068899226901324,
    0.21138672530651093, 0.22884385611699976, 0.20682305032792297, 0.22591188792949138, 0.19981038165481194,
    0.20695534640032312, 0.19852250441908836, 0.19030446688766064, 0.18669014569857847, 0.18753752970825072,
    0.18753858202177545, 0.1894801406432753, 0.19126981901733772, 0.20132101035636404, 0.18867324308856673,
    0.20866803419978722, 0.2084117426496485, 0.2019648517522475, 0.20446901671264484, 0.2143265099629112,
    0.23850845302576604, 0.2411397818637931, 0.24226528616703075, 0.24621676783198895, 0.2199540713235088,
    0.22196668467443922, 0.33460922664760245, 0.2857733377781899, 0.21751534493397112, 0.28228427510222664,
    0.20490934505410816, 0.20631158837805624, 0.21445682681287112, 0.2676555684403233, 0.1972119222678568,
    0.23883166698657948, 0.2929570472434811, 0.19198378580419914,
]

val_accuracy = [
    0.6565096952908587, 0.8337950138504155, 0.8878116343490304, 0.9058171745152355, 0.8878116343490304,
    0.9238227146814404, 0.9445983379501385, 0.9390581717451524, 0.9335180055401662, 0.9418282548476454,
    0.9445983379501385, 0.9390581717451524, 0.9418282548476454, 0.945983379501385, 0.9445983379501385,
    0.9515235457063712, 0.9473684210526315, 0.9570637119113573, 0.9570637119113573, 0.9570637119113573,
    0.9570637119113573, 0.9570637119113573, 0.9570637119113573, 0.9584487534626038, 0.9556786703601108,
    0.9529085872576177, 0.9445983379501385, 0.9445983379501385, 0.9542936288088643, 0.9376731301939059,
    0.925207756232687, 0.9362880886426593, 0.9335180055401662, 0.9279778393351801, 0.9307479224376731,
    0.9279778393351801, 0.9016620498614959, 0.9196675900277008, 0.925207756232687, 0.9155124653739612,
    0.9362880886426593, 0.9404432132963989, 0.9376731301939059, 0.9085872576177285, 0.943213296398892,
    0.9307479224376731, 0.9141274238227147, 0.945983379501385, 0.9279778393351801, 0.945983379501385,
    0.9390581717451524, 0.9487534626038782, 0.945983379501385, 0.9515235457063712, 0.945983379501385,
    0.943213296398892, 0.9390581717451524, 0.9501385041551247, 0.9501385041551247, 0.9501385041551247,
    0.9501385041551247, 0.9473684210526315, 0.9487534626038782, 0.9445983379501385, 0.9529085872576177,
    0.9487534626038782, 0.9418282548476454, 0.9501385041551247, 0.9445983379501385, 0.9445983379501385,
    0.9473684210526315, 0.9390581717451524, 0.945983379501385, 0.9155124653739612, 0.9418282548476454,
    0.9335180055401662, 0.9418282548476454, 0.9404432132963989, 0.9404432132963989, 0.925207756232687,
    0.9210526315789473, 0.9016620498614959, 0.9404432132963989, 0.9127423822714681, 0.9293628808864266,
    0.9362880886426593, 0.9390581717451524, 0.9362880886426593, 0.9349030470914127, 0.9321329639889196,
    0.9390581717451524, 0.9501385041551247, 0.9473684210526315, 0.943213296398892, 0.9487534626038782,
    0.9487534626038782, 0.9515235457063712, 0.9487534626038782, 0.9501385041551247, 0.9501385041551247
]
# Plotting training loss
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_loss, label='Training Loss', color='tab:blue')
plt.title('Training Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('training_loss.png', dpi=300)
plt.show()

# Plotting validation accuracy
plt.figure(figsize=(10, 6))
plt.plot(epochs, val_accuracy, label='Validation Accuracy', color='tab:red')
plt.title('Validation Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('validation_accuracy.png', dpi=300)
plt.show()
